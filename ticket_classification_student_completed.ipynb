{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "unsYXGRTdHg0"
      },
      "source": [
        "# Deep Learning NLP Project: Customer Support Ticket Classification\n",
        "## Student Practice Notebook\n",
        "\n",
        "### Project Overview\n",
        "In this project, you will build an end-to-end Deep Learning solution for automatically classifying customer support tickets into appropriate departments using Natural Language Processing (NLP) techniques. You'll create a hybrid CNN-LSTM neural network model to categorize support tickets into departments (Technical Support, Sales, Billing, Customer Service).\n",
        "\n",
        "### Learning Objectives:\n",
        "- Understand text classification for business applications\n",
        "- Learn hybrid CNN-LSTM architectures\n",
        "- Master text preprocessing for support tickets\n",
        "- Build and deploy a ticket routing system\n",
        "- Evaluate multi-class classification performance\n",
        "\n",
        "### Business Value:\n",
        "- Automatic ticket routing to appropriate departments\n",
        "- Faster customer response times\n",
        "- Improved support team efficiency\n",
        "- Better resource allocation\n",
        "\n",
        "### Instructions:\n",
        "Complete each code cell by writing the required code based on the comments provided. Each cell contains detailed instructions about what needs to be implemented."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NeWGgMOdHg1"
      },
      "source": [
        "## Step 1: Import Required Libraries\n",
        "\n",
        "**Task:** Import all necessary libraries for data processing, NLP, and deep learning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r-SF4rtNdHg1",
        "outputId": "65b3e10f-dae0-4f38-ed97-94086e07e653"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All libraries imported successfully!\n",
            "TensorFlow version: 2.19.0\n"
          ]
        }
      ],
      "source": [
        "# Data manipulation and analysis\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import string\n",
        "import pickle\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# NLP libraries\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Download required NLTK data\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('punkt_tab', quiet=True)\n",
        "nltk.download('stopwords', quiet=True)\n",
        "nltk.download('wordnet', quiet=True)\n",
        "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
        "\n",
        "# Sklearn for preprocessing and metrics\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "\n",
        "# TensorFlow and Keras for Deep Learning\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional, GlobalMaxPooling1D, Conv1D, MaxPooling1D\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "print(\"All libraries imported successfully!\")\n",
        "print(f\"TensorFlow version: {tf.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqQh2HwvdHg2"
      },
      "source": [
        "## Step 2: Load and Explore Dataset\n",
        "\n",
        "**Task:** Load the support tickets CSV file and perform exploratory data analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gxiksELYdHg2",
        "outputId": "35fc35c5-427f-4ec1-ce7a-e3ca593926f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 165 entries, 0 to 164\n",
            "Data columns (total 4 columns):\n",
            " #   Column       Non-Null Count  Dtype \n",
            "---  ------       --------------  ----- \n",
            " 0   ticket_text  165 non-null    object\n",
            " 1   priority     165 non-null    object\n",
            " 2   department   165 non-null    object\n",
            " 3   status       165 non-null    object\n",
            "dtypes: object(4)\n",
            "memory usage: 5.3+ KB\n",
            "department\n",
            "Technical Support    68\n",
            "Customer Service     50\n",
            "Sales                33\n",
            "Billing              14\n",
            "Name: count, dtype: int64\n",
            "priority\n",
            "High      67\n",
            "Low       55\n",
            "Medium    43\n",
            "Name: count, dtype: int64\n",
            "status\n",
            "Open      134\n",
            "Closed     31\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Load the dataset from 'support_tickets.csv' into a DataFrame called df\n",
        "df = pd.read_csv(\"/content/support_tickets.csv\")\n",
        "\n",
        "# Print the shape of the dataset with label \"Dataset Shape:\"\n",
        "df.shape\n",
        "\n",
        "# Print \"\\nFirst few rows:\" and display the first 10 rows using head(10)\n",
        "df.head(10)\n",
        "\n",
        "\n",
        "# Print \"\\nDataset Info:\" and display info about the dataset\n",
        "df.info()\n",
        "\n",
        "\n",
        "# Print \"\\nMissing values:\" and display count of missing values for each column\n",
        "df.isnull().sum()\n",
        "\n",
        "\n",
        "# Print \"\\nDepartment distribution:\" and display value counts of 'department' column\n",
        "print(df[\"department\"].value_counts())\n",
        "\n",
        "\n",
        "# Print \"\\nPriority distribution:\" and display value counts of 'priority' column\n",
        "print(df[\"priority\"].value_counts())\n",
        "\n",
        "\n",
        "# Print \"\\nStatus distribution:\" and display value counts of 'status' column\n",
        "print(df[\"status\"].value_counts())\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2r_zaAUUdHg2"
      },
      "source": [
        "## Step 3: Data Cleaning and Preprocessing\n",
        "\n",
        "**Task:** Clean the data by removing duplicates and handling missing values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wSq5zENadHg2",
        "outputId": "ba999fcd-713b-47fd-8638-797be98e27d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "0\n",
            "(165, 4)\n",
            "\n",
            "Department distribution after cleaning:\n",
            "department\n",
            "Technical Support    68\n",
            "Customer Service     50\n",
            "Sales                33\n",
            "Billing              14\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Create a copy of df called df_clean\n",
        "df_clean = df.copy()\n",
        "\n",
        "# Print the number of duplicates before removal using duplicated().sum()\n",
        "print(df_clean.duplicated().sum())\n",
        "\n",
        "# Remove duplicates based on 'ticket_text' column, keeping the first occurrence\n",
        "df_clean = df_clean.drop_duplicates(subset='ticket_text', keep='first')\n",
        "\n",
        "# Print the number of duplicates after removal\n",
        "print(df_clean.duplicated().sum())\n",
        "\n",
        "# Drop rows with missing values in 'ticket_text' or 'department' columns\n",
        "df_clean = df_clean.dropna(subset=[\"ticket_text\", \"department\"])\n",
        "\n",
        "# Reset the index of df_clean, dropping the old index\n",
        "df_clean.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# Print the final dataset shape\n",
        "print(df_clean.shape)\n",
        "\n",
        "# Print \"\\nDepartment distribution after cleaning:\" and display value counts of 'department' column\n",
        "print(\"\\nDepartment distribution after cleaning:\")\n",
        "print(df_clean['department'].value_counts())\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQb6131gdHg2"
      },
      "source": [
        "## Step 4: Text Preprocessing Functions\n",
        "\n",
        "**Task:** Create functions to clean and preprocess support ticket text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t3j2SthcdHg2",
        "outputId": "ce6e77e0-ec50-434e-f64a-35ef4efffe45"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenization and lemmatization function defined successfully!\n"
          ]
        }
      ],
      "source": [
        "# Initialize WordNetLemmatizer and store in variable called lemmatizer\n",
        "\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Keep some important words for support tickets\n",
        "important_words = {'not', 'no', 'nor', 'very', 'urgent', 'immediately', 'cannot', 'need', 'help', 'issue', 'problem'}\n",
        "stop_words = stop_words - important_words\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"\n",
        "    Clean and preprocess text data\n",
        "    Steps:\n",
        "    1. Convert to lowercase\n",
        "    2. Remove URLs\n",
        "    3. Remove HTML tags\n",
        "    4. Remove email addresses\n",
        "    5. Remove special characters and digits\n",
        "    6. Remove extra whitespace\n",
        "    \"\"\"\n",
        "    # Convert to lowercase\n",
        "    text = str(text).lower()\n",
        "\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "\n",
        "    # Remove HTML tags\n",
        "    text = re.sub(r'<.*?>', '', text)\n",
        "\n",
        "    # Remove email addresses\n",
        "    text = re.sub(r'\\S+@\\S+', '', text)\n",
        "\n",
        "    # Remove order numbers and ticket IDs\n",
        "    text = re.sub(r'#\\d+', '', text)\n",
        "\n",
        "    # Remove special characters and digits but keep apostrophes\n",
        "    text = re.sub(r\"[^a-zA-Z\\s']\", \" \", text)\n",
        "\n",
        "\n",
        "    # Remove extra whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Define a function tokenize_and_lemmatize(text) that:\n",
        "# 1. Tokenizes the text using word_tokenize()\n",
        "# 2. Lemmatizes each token using lemmatizer.lemmatize() if token is not in stop_words and length > 2\n",
        "# 3. Joins the tokens back into a string with spaces\n",
        "# 4. Returns the processed text\n",
        "\n",
        "def tokenize_and_lemmatize(text):\n",
        "      tokens = word_tokenize(text)\n",
        "      # Lemmatize and remove stopwords + short words\n",
        "      processed_tokens = [\n",
        "          lemmatizer.lemmatize(token)\n",
        "          for token in tokens\n",
        "          if token not in stop_words and len(token) > 2\n",
        "      ]\n",
        "      # Join tokens back into string\n",
        "      processed_text = \" \".join(processed_tokens)\n",
        "      return processed_text\n",
        "\n",
        "# Print success message\n",
        "\n",
        "print(\"Tokenization and lemmatization function defined successfully!\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Print success message\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZFjhrHxCdHg2"
      },
      "source": [
        "## Step 5: Apply Text Preprocessing\n",
        "\n",
        "**Task:** Apply the preprocessing functions to all support tickets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nHZBKFkudHg2",
        "outputId": "1c0c7328-b285-4f9e-d30e-e14c18ba68a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaning Text\n",
            "Tokenizing and lemmatizing...\n",
            "Original: My internet connection keeps dropping every few minutes. This has been happening for the past 3 days. Very frustrating!\n",
            "\n",
            "Cleaned: my internet connection keeps dropping every few minutes this has been happening for the past days very frustrating\n",
            "\n",
            "Processed: internet connection keep dropping every minute happening past day very frustrating\n",
            "Final dataset shape: (165, 6)\n"
          ]
        }
      ],
      "source": [
        "# Print \"Cleaning text...\"\n",
        "print(\"Cleaning Text\")\n",
        "\n",
        "# Create a new column 'cleaned_text' by applying clean_text function to 'ticket_text' column\n",
        "df_clean['cleaned_text'] = df_clean['ticket_text'].apply(clean_text)\n",
        "\n",
        "# Print \"Tokenizing and lemmatizing...\"\n",
        "print(\"Tokenizing and lemmatizing...\")\n",
        "\n",
        "# Create a new column 'processed_text' by applying tokenize_and_lemmatize function to 'cleaned_text' column\n",
        "df_clean['processed_text'] = df_clean['cleaned_text'].apply(tokenize_and_lemmatize)\n",
        "\n",
        "# Remove rows where 'processed_text' is empty (after stripping whitespace)\n",
        "df_clean = df_clean[df_clean['processed_text'].str.strip() != '']\n",
        "\n",
        "# Reset the index, dropping the old index\n",
        "df_clean = df_clean.reset_index(drop=True)\n",
        "\n",
        "# Print example showing original, cleaned, and processed text for the first row\n",
        "# Format: \"Original: {original_text}\\n\\nCleaned: {cleaned_text}\\n\\nProcessed: {processed_text}\"\n",
        "row = df_clean.iloc[0]\n",
        "print(f\"Original: {row['ticket_text']}\\n\\nCleaned: {row['cleaned_text']}\\n\\nProcessed: {row['processed_text']}\")\n",
        "\n",
        "\n",
        "\n",
        "# Print the final dataset shape after preprocessing\n",
        "print(f\"Final dataset shape: {df_clean.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rxm52DqddHg2"
      },
      "source": [
        "## Step 6: Feature Engineering\n",
        "\n",
        "**Task:** Create additional features from the support ticket text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KCz0jfPwdHg2",
        "outputId": "eb77f6a4-0b41-461c-ad44-f303079ea285"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text statistics:\n",
            "Average text length: 73.32\n",
            "Average word count: 9.56\n",
            "Average word length: 6.85\n",
            "Average exclamation count: 0.46\n",
            "Average question count: 0.19\n"
          ]
        }
      ],
      "source": [
        "# Create a new column 'text_length' with the length of 'processed_text' for each row\n",
        "df_clean['text_length'] = df_clean['processed_text'].str.len()\n",
        "\n",
        "# Create a new column 'word_count' with the count of words (split by space) in 'processed_text'\n",
        "df_clean['word_count'] = df_clean['processed_text'].str.split().str.len()\n",
        "\n",
        "# Create a new column 'avg_word_length' with the average length of words in 'processed_text'\n",
        "# Use a lambda function that calculates mean of word lengths, or 0 if no words exist\n",
        "df_clean['avg_word_length'] = df_clean['processed_text'].apply(\n",
        "    lambda x: sum(len(w) for w in x.split()) / len(x.split()) if len(x.split()) > 0 else 0\n",
        ")\n",
        "\n",
        "# Create a new column 'exclamation_count' counting exclamation marks in 'ticket_text' (urgency indicator)\n",
        "df_clean['exclamation_count'] = df_clean['ticket_text'].str.count('!')\n",
        "\n",
        "# Create a new column 'question_count' counting question marks in 'ticket_text'\n",
        "df_clean['question_count'] = df_clean['ticket_text'].str.count('\\?')\n",
        "\n",
        "# Print \"Text statistics:\" header\n",
        "print(\"Text statistics:\")\n",
        "\n",
        "# Print average text_length with 2 decimal places\n",
        "print(f\"Average text length: {df_clean['text_length'].mean():.2f}\")\n",
        "\n",
        "# Print average word_count with 2 decimal places\n",
        "print(f\"Average word count: {df_clean['word_count'].mean():.2f}\")\n",
        "\n",
        "# Print average avg_word_length with 2 decimal places\n",
        "print(f\"Average word length: {df_clean['avg_word_length'].mean():.2f}\")\n",
        "\n",
        "# Print average exclamation_count with 2 decimal places\n",
        "print(f\"Average exclamation count: {df_clean['exclamation_count'].mean():.2f}\")\n",
        "\n",
        "# Print average question_count with 2 decimal places\n",
        "print(f\"Average question count: {df_clean['question_count'].mean():.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TYCBPzPSdHg3"
      },
      "source": [
        "## Step 7: Prepare Data for Deep Learning Model\n",
        "\n",
        "**Task:** Prepare features and labels, encode department labels for neural network training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z1RjKfeudHg3",
        "outputId": "0c61d2a4-3dda-4bdc-b69e-b00a65bb7690"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label mapping:\n",
            "0: Billing\n",
            "1: Customer Service\n",
            "2: Sales\n",
            "3: Technical Support\n",
            "Number of classes: 4\n",
            "Shape of X: (165,)\n",
            "Shape of y_categorical: (165, 4)\n"
          ]
        }
      ],
      "source": [
        "# Create X as a numpy array from 'processed_text' column values\n",
        "X = df_clean['processed_text'].values\n",
        "\n",
        "# Create y as a numpy array from 'department' column values\n",
        "y = df_clean['department'].values\n",
        "\n",
        "# Initialize a LabelEncoder object\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# Fit and transform y using the label_encoder, store result in y_encoded\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "# Print \"Label mapping:\" header\n",
        "print(\"Label mapping:\")\n",
        "\n",
        "# Loop through label_encoder.classes_ with enumerate and print each department label with its index\n",
        "for index, label in enumerate(label_encoder.classes_):\n",
        "    print(f\"{index}: {label}\")\n",
        "\n",
        "\n",
        "# Convert y_encoded to categorical format using to_categorical(), store in y_categorical\n",
        "y_categorical = to_categorical(y_encoded)\n",
        "\n",
        "# Get the number of classes from the shape of y_categorical (second dimension)\n",
        "num_classes = y_categorical.shape[1]\n",
        "\n",
        "# Print the number of classes (departments)\n",
        "print(f\"Number of classes: {num_classes}\")\n",
        "\n",
        "# Print the shape of X (features)\n",
        "print(f\"Shape of X: {X.shape}\")\n",
        "\n",
        "# Print the shape of y_categorical (labels)\n",
        "print(f\"Shape of y_categorical: {y_categorical.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qCWHfhHKdHg3"
      },
      "source": [
        "## Step 8: Text Tokenization and Padding\n",
        "\n",
        "**Task:** Convert text to sequences and pad them to uniform length."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7lQEKlc0dHg3",
        "outputId": "c4883d42-14e0-4aee-826f-55c2c8643b71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 800\n",
            "Shape of X_padded: (165, 150)\n",
            "First 10 tokens of sequence 0: [282 150 151 283  90 284 285 152 286   9]\n"
          ]
        }
      ],
      "source": [
        "# Define MAX_WORDS constant as 5000 (maximum vocabulary size)\n",
        "MAX_WORDS = 5000\n",
        "\n",
        "# Define MAX_SEQUENCE_LENGTH constant as 150 (support tickets are typically longer)\n",
        "MAX_SEQUENCE_LENGTH = 150\n",
        "\n",
        "# Initialize a Tokenizer with num_words=MAX_WORDS and oov_token='<OOV>'\n",
        "tokenizer = Tokenizer(num_words=MAX_WORDS, oov_token='<OOV>')\n",
        "\n",
        "# Fit the tokenizer on texts in X\n",
        "tokenizer.fit_on_texts(X)\n",
        "\n",
        "# Convert texts to sequences using tokenizer.texts_to_sequences()\n",
        "X_sequences = tokenizer.texts_to_sequences(X)\n",
        "\n",
        "# Pad sequences to MAX_SEQUENCE_LENGTH using pad_sequences with padding='post' and truncating='post'\n",
        "X_padded = pad_sequences(X_sequences, maxlen=MAX_SEQUENCE_LENGTH, padding='post', truncating='post')\n",
        "\n",
        "# Print the vocabulary size (length of tokenizer.word_index)\n",
        "print(f\"Vocabulary size: {len(tokenizer.word_index)}\")\n",
        "\n",
        "# Print the shape of X_padded\n",
        "print(f\"Shape of X_padded: {X_padded.shape}\")\n",
        "\n",
        "# Print example: first 10 tokens of the first padded sequence\n",
        "print(f\"First 10 tokens of sequence 0: {X_padded[0][:10]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IiwBC2NPdHg3"
      },
      "source": [
        "## Step 9: Train-Test Split\n",
        "\n",
        "**Task:** Split the data into training and testing sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jxGMc8pydHg3",
        "outputId": "fdac7546-74ae-474b-9bd4-833b500e8656"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set size: 132\n",
            "Testing set size: 33\n",
            "Shape of X_train: (132, 150)\n",
            "Shape of X_test: (33, 150)\n"
          ]
        }
      ],
      "source": [
        "# Split X_padded and y_categorical into train and test sets\n",
        "# Use test_size=0.2, random_state=42, and stratify=y_encoded\n",
        "# Store results in X_train, X_test, y_train, y_test\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_padded,\n",
        "    y_categorical,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=y_encoded\n",
        ")\n",
        "\n",
        "\n",
        "# Print training set size (number of samples in X_train)\n",
        "print(f\"Training set size: {len(X_train)}\")\n",
        "\n",
        "# Print testing set size (number of samples in X_test)\n",
        "print(f\"Testing set size: {len(X_test)}\")\n",
        "\n",
        "# Print the shape of X_train\n",
        "print(f\"Shape of X_train: {X_train.shape}\")\n",
        "\n",
        "# Print the shape of X_test\n",
        "print(f\"Shape of X_test: {X_test.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D5kjiVBZdHg3"
      },
      "source": [
        "## Step 10: Build Hybrid CNN-LSTM Deep Learning Model\n",
        "\n",
        "**Task:** Create a Sequential model combining CNN and LSTM layers for better ticket classification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 518
        },
        "id": "_hEVdfixdHg3",
        "outputId": "85f4b402-0cd7-4837-d4d3-608efe4b7be0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Architecture:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv1d (\u001b[38;5;33mConv1D\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling1d (\u001b[38;5;33mMaxPooling1D\u001b[0m)    │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ bidirectional (\u001b[38;5;33mBidirectional\u001b[0m)   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_max_pooling1d            │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mGlobalMaxPooling1D\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)    │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ bidirectional (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_max_pooling1d            │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling1D</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Define EMBEDDING_DIM constant as 128\n",
        "EMBEDDING_DIM = 128\n",
        "\n",
        "# Define LSTM_UNITS constant as 64\n",
        "LSTM_UNITS = 64\n",
        "\n",
        "# Define CNN_FILTERS constant as 64\n",
        "CNN_FILTERS = 64\n",
        "\n",
        "# Create a Sequential model with the following layers:\n",
        "# 1. Embedding layer: input_dim=MAX_WORDS, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH\n",
        "# 2. Conv1D layer: filters=CNN_FILTERS, kernel_size=5, activation='relu'\n",
        "# 3. MaxPooling1D layer: pool_size=2\n",
        "# 4. Bidirectional LSTM layer: LSTM_UNITS units, return_sequences=True\n",
        "# 5. Dropout layer: rate=0.5\n",
        "# 6. GlobalMaxPooling1D layer\n",
        "# 7. Dense layer: 128 units, activation='relu'\n",
        "# 8. Dropout layer: rate=0.5\n",
        "# 9. Dense layer: 64 units, activation='relu'\n",
        "# 10. Dropout layer: rate=0.3\n",
        "# 11. Dense layer: num_classes units, activation='softmax'\n",
        "\n",
        "\n",
        "model = Sequential([\n",
        "    # 1. Embedding layer\n",
        "    Embedding(input_dim=MAX_WORDS, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH),\n",
        "\n",
        "    # 2. Conv1D layer for local feature extraction\n",
        "    Conv1D(filters=CNN_FILTERS, kernel_size=5, activation='relu'),\n",
        "\n",
        "    # 3. MaxPooling1D layer to reduce dimensionality\n",
        "    MaxPooling1D(pool_size=2),\n",
        "\n",
        "    # 4. Bidirectional LSTM layer for sequence dependency\n",
        "    Bidirectional(LSTM(LSTM_UNITS, return_sequences=True)),\n",
        "\n",
        "    # 5. Dropout to prevent overfitting\n",
        "    Dropout(0.5),\n",
        "\n",
        "    # 6. GlobalMaxPooling1D to condense the sequence into a single vector\n",
        "    GlobalMaxPooling1D(),\n",
        "\n",
        "    # 7. Dense layer for deeper feature processing\n",
        "    Dense(128, activation='relu'),\n",
        "\n",
        "    # 8. Dropout\n",
        "    Dropout(0.5),\n",
        "\n",
        "    # 9. Dense layer\n",
        "    Dense(64, activation='relu'),\n",
        "\n",
        "    # 10. Dropout\n",
        "    Dropout(0.3),\n",
        "\n",
        "    # 11. Final Dense layer with softmax for multi-class classification\n",
        "    Dense(num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Compile the model with:\n",
        "# optimizer='adam'\n",
        "# loss='categorical_crossentropy'\n",
        "# metrics=['accuracy']\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Print \"Model Architecture:\" and display model summary\n",
        "print(\"Model Architecture:\")\n",
        "model.summary()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_iYDatqldHg3"
      },
      "source": [
        "## Step 11: Train the Model\n",
        "\n",
        "**Task:** Set up callbacks and train the hybrid CNN-LSTM model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yo1nB1MidHg3",
        "outputId": "ec14a278-b2b0-411c-fce6-076ac31237eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training the model...\n",
            "Epoch 1/30\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - accuracy: 0.2557 - loss: 1.3833 \n",
            "Epoch 1: val_accuracy improved from -inf to 0.48148, saving model to best_ticket_model.keras\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 343ms/step - accuracy: 0.2560 - loss: 1.3834 - val_accuracy: 0.4815 - val_loss: 1.3631 - learning_rate: 0.0010\n",
            "Epoch 2/30\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - accuracy: 0.3342 - loss: 1.3537\n",
            "Epoch 2: val_accuracy did not improve from 0.48148\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 133ms/step - accuracy: 0.3378 - loss: 1.3539 - val_accuracy: 0.4815 - val_loss: 1.3336 - learning_rate: 0.0010\n",
            "Epoch 3/30\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - accuracy: 0.3882 - loss: 1.3238\n",
            "Epoch 3: val_accuracy did not improve from 0.48148\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 137ms/step - accuracy: 0.3868 - loss: 1.3264 - val_accuracy: 0.4815 - val_loss: 1.2991 - learning_rate: 0.0010\n",
            "Epoch 4/30\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - accuracy: 0.3268 - loss: 1.3009\n",
            "Epoch 4: val_accuracy did not improve from 0.48148\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 139ms/step - accuracy: 0.3281 - loss: 1.3016 - val_accuracy: 0.4815 - val_loss: 1.2831 - learning_rate: 0.0010\n",
            "Epoch 5/30\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - accuracy: 0.3726 - loss: 1.2735 \n",
            "Epoch 5: val_accuracy did not improve from 0.48148\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 128ms/step - accuracy: 0.3743 - loss: 1.2738 - val_accuracy: 0.4815 - val_loss: 1.2745 - learning_rate: 0.0010\n",
            "Epoch 6/30\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - accuracy: 0.3713 - loss: 1.2710\n",
            "Epoch 6: val_accuracy did not improve from 0.48148\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 132ms/step - accuracy: 0.3732 - loss: 1.2753 - val_accuracy: 0.4815 - val_loss: 1.2829 - learning_rate: 0.0010\n",
            "Epoch 7/30\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - accuracy: 0.3576 - loss: 1.2622 \n",
            "Epoch 7: val_accuracy did not improve from 0.48148\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 126ms/step - accuracy: 0.3566 - loss: 1.2680 - val_accuracy: 0.4815 - val_loss: 1.2980 - learning_rate: 0.0010\n",
            "Epoch 8/30\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - accuracy: 0.4455 - loss: 1.2565\n",
            "Epoch 8: val_accuracy did not improve from 0.48148\n",
            "\n",
            "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 141ms/step - accuracy: 0.4440 - loss: 1.2575 - val_accuracy: 0.4815 - val_loss: 1.2954 - learning_rate: 0.0010\n",
            "Epoch 9/30\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - accuracy: 0.4225 - loss: 1.2259 \n",
            "Epoch 9: val_accuracy did not improve from 0.48148\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 132ms/step - accuracy: 0.4218 - loss: 1.2255 - val_accuracy: 0.4815 - val_loss: 1.2839 - learning_rate: 5.0000e-04\n",
            "Epoch 10/30\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - accuracy: 0.3721 - loss: 1.2287\n",
            "Epoch 10: val_accuracy did not improve from 0.48148\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 141ms/step - accuracy: 0.3777 - loss: 1.2282 - val_accuracy: 0.4815 - val_loss: 1.2649 - learning_rate: 5.0000e-04\n",
            "Epoch 11/30\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - accuracy: 0.4220 - loss: 1.1840 \n",
            "Epoch 11: val_accuracy did not improve from 0.48148\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 130ms/step - accuracy: 0.4252 - loss: 1.1830 - val_accuracy: 0.4815 - val_loss: 1.2423 - learning_rate: 5.0000e-04\n",
            "Epoch 12/30\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - accuracy: 0.4544 - loss: 1.1012 \n",
            "Epoch 12: val_accuracy did not improve from 0.48148\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 130ms/step - accuracy: 0.4530 - loss: 1.0993 - val_accuracy: 0.4815 - val_loss: 1.2109 - learning_rate: 5.0000e-04\n",
            "Epoch 13/30\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - accuracy: 0.4426 - loss: 1.0471 \n",
            "Epoch 13: val_accuracy did not improve from 0.48148\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 126ms/step - accuracy: 0.4436 - loss: 1.0436 - val_accuracy: 0.4815 - val_loss: 1.1749 - learning_rate: 5.0000e-04\n",
            "Epoch 14/30\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - accuracy: 0.5522 - loss: 0.9838 \n",
            "Epoch 14: val_accuracy did not improve from 0.48148\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 128ms/step - accuracy: 0.5522 - loss: 0.9796 - val_accuracy: 0.4815 - val_loss: 1.1358 - learning_rate: 5.0000e-04\n",
            "Epoch 15/30\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.5990 - loss: 0.8990\n",
            "Epoch 15: val_accuracy improved from 0.48148 to 0.51852, saving model to best_ticket_model.keras\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 218ms/step - accuracy: 0.6011 - loss: 0.8983 - val_accuracy: 0.5185 - val_loss: 1.0975 - learning_rate: 5.0000e-04\n",
            "Epoch 16/30\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 177ms/step - accuracy: 0.6747 - loss: 0.8606\n",
            "Epoch 16: val_accuracy did not improve from 0.51852\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 251ms/step - accuracy: 0.6712 - loss: 0.8576 - val_accuracy: 0.5185 - val_loss: 1.0712 - learning_rate: 5.0000e-04\n",
            "Epoch 17/30\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - accuracy: 0.6443 - loss: 0.8401\n",
            "Epoch 17: val_accuracy did not improve from 0.51852\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 240ms/step - accuracy: 0.6507 - loss: 0.8373 - val_accuracy: 0.5185 - val_loss: 1.0571 - learning_rate: 5.0000e-04\n",
            "Epoch 18/30\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - accuracy: 0.7087 - loss: 0.8146 \n",
            "Epoch 18: val_accuracy did not improve from 0.51852\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 127ms/step - accuracy: 0.7079 - loss: 0.8135 - val_accuracy: 0.4074 - val_loss: 1.0617 - learning_rate: 5.0000e-04\n",
            "Epoch 19/30\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - accuracy: 0.7176 - loss: 0.7866 \n",
            "Epoch 19: val_accuracy did not improve from 0.51852\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 127ms/step - accuracy: 0.7170 - loss: 0.7849 - val_accuracy: 0.4815 - val_loss: 1.0906 - learning_rate: 5.0000e-04\n",
            "Epoch 20/30\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - accuracy: 0.7393 - loss: 0.7488 \n",
            "Epoch 20: val_accuracy did not improve from 0.51852\n",
            "\n",
            "Epoch 20: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 127ms/step - accuracy: 0.7381 - loss: 0.7485 - val_accuracy: 0.4815 - val_loss: 1.1223 - learning_rate: 5.0000e-04\n",
            "Epoch 21/30\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - accuracy: 0.7317 - loss: 0.7340 \n",
            "Epoch 21: val_accuracy did not improve from 0.51852\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 127ms/step - accuracy: 0.7301 - loss: 0.7341 - val_accuracy: 0.4815 - val_loss: 1.1272 - learning_rate: 2.5000e-04\n",
            "Epoch 22/30\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - accuracy: 0.7354 - loss: 0.6788 \n",
            "Epoch 22: val_accuracy did not improve from 0.51852\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 128ms/step - accuracy: 0.7350 - loss: 0.6806 - val_accuracy: 0.4815 - val_loss: 1.1282 - learning_rate: 2.5000e-04\n",
            "Epoch 22: early stopping\n",
            "Restoring model weights from the end of the best epoch: 17.\n",
            "\n",
            "Training completed!\n"
          ]
        }
      ],
      "source": [
        "# Create an EarlyStopping callback with:\n",
        "# monitor='val_loss', patience=5, restore_best_weights=True, verbose=1\n",
        "\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=5,\n",
        "    restore_best_weights=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "# Create a ModelCheckpoint callback with:\n",
        "# filepath='best_ticket_model.keras', monitor='val_accuracy', save_best_only=True, verbose=1\n",
        "checkpoint = ModelCheckpoint(\n",
        "    filepath='best_ticket_model.keras',\n",
        "    monitor='val_accuracy',\n",
        "    save_best_only=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Create a ReduceLROnPlateau callback with:\n",
        "# monitor='val_loss', factor=0.5, patience=3, min_lr=0.00001, verbose=1\n",
        "reduce_lr = ReduceLROnPlateau(\n",
        "    monitor='val_loss',\n",
        "    factor=0.5,\n",
        "    patience=3,\n",
        "    min_lr=0.00001,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Print \"Training the model...\"\n",
        "print(\"Training the model...\")\n",
        "\n",
        "# Train the model using fit() with:\n",
        "# X_train, y_train as data\n",
        "# epochs=30\n",
        "# batch_size=32\n",
        "# validation_split=0.2\n",
        "# callbacks=[early_stopping, model_checkpoint, reduce_lr]\n",
        "# verbose=1\n",
        "# Store the result in a variable called history\n",
        "history = model.fit(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    epochs=30,\n",
        "    batch_size=32,\n",
        "    validation_split=0.2,\n",
        "    callbacks=[early_stopping, checkpoint, reduce_lr],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Print \"\\nTraining completed!\"\n",
        "print(\"\\nTraining completed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "URkjkFX7dHg3"
      },
      "source": [
        "## Step 12: Evaluate the Model\n",
        "\n",
        "**Task:** Evaluate model performance on test data and display detailed metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6WeEHbdbdHg3",
        "outputId": "ddc80d07-9b75-415d-aa57-d71d1e5ce31c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 1.0715\n",
            "Test Accuracy: 0.6061\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 637ms/step\n",
            "\n",
            "Classification Report:\n",
            "                   precision    recall  f1-score   support\n",
            "\n",
            "          Billing       0.00      0.00      0.00         3\n",
            " Customer Service       0.43      0.60      0.50        10\n",
            "            Sales       0.00      0.00      0.00         6\n",
            "Technical Support       0.74      1.00      0.85        14\n",
            "\n",
            "         accuracy                           0.61        33\n",
            "        macro avg       0.29      0.40      0.34        33\n",
            "     weighted avg       0.44      0.61      0.51        33\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[ 0  2  0  1]\n",
            " [ 0  6  0  4]\n",
            " [ 0  6  0  0]\n",
            " [ 0  0  0 14]]\n",
            "\n",
            "Confusion Matrix with Labels:\n",
            "                   Billing  Customer Service  Sales  Technical Support\n",
            "Billing                  0                 2      0                  1\n",
            "Customer Service         0                 6      0                  4\n",
            "Sales                    0                 6      0                  0\n",
            "Technical Support        0                 0      0                 14\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the model on X_test and y_test with verbose=0\n",
        "# Store the results in test_loss and test_accuracy\n",
        "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
        "\n",
        "# Print the test loss with 4 decimal places\n",
        "print(f\"Test Loss: {test_loss:.4f}\")\n",
        "\n",
        "# Print the test accuracy with 4 decimal places\n",
        "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
        "\n",
        "# Make predictions on X_test using model.predict(), store in y_pred_proba\n",
        "y_pred_proba = model.predict(X_test)\n",
        "\n",
        "# Get the predicted class by finding argmax along axis=1, store in y_pred\n",
        "y_pred = np.argmax(y_pred_proba, axis=1)\n",
        "\n",
        "# Get the true class by finding argmax of y_test along axis=1, store in y_true\n",
        "y_true = np.argmax(y_test, axis=1)\n",
        "\n",
        "# Print \"\\nClassification Report:\" header\n",
        "print(\"\\nClassification Report:\")\n",
        "\n",
        "# Print the classification report using y_true and y_pred with target_names from label_encoder.classes_\n",
        "print(classification_report(y_true, y_pred, target_names=label_encoder.classes_))\n",
        "\n",
        "\n",
        "# Print \"\\nConfusion Matrix:\" header\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "\n",
        "# Calculate confusion matrix using y_true and y_pred, store in cm\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "# Print the confusion matrix\n",
        "print(cm)\n",
        "\n",
        "# Create a DataFrame cm_df from cm with index and columns as label_encoder.classes_\n",
        "cm_df = pd.DataFrame(\n",
        "    cm,\n",
        "    index=label_encoder.classes_,\n",
        "    columns=label_encoder.classes_\n",
        ")\n",
        "\n",
        "# Print \"\\nConfusion Matrix with Labels:\" header\n",
        "print(\"\\nConfusion Matrix with Labels:\")\n",
        "\n",
        "# Print the labeled confusion matrix DataFrame\n",
        "print(cm_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iz12SK6PdHg3"
      },
      "source": [
        "## Step 13: Save Model and Required Objects\n",
        "\n",
        "**Task:** Save all artifacts needed for deployment in the Streamlit application."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aJWpelM1dHg3",
        "outputId": "a2f7ea4a-d76e-4c6a-d315-008b32dc209a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved as 'ticket_classifier_model.keras'\n",
            "Tokenizer saved as 'ticket_tokenizer.pkl'\n",
            "Label encoder saved as 'ticket_label_encoder.pkl'\n",
            "Preprocessing parameters saved as 'ticket_preprocessing_params.pkl'\n",
            "\n",
            "All artifacts saved successfully!\n"
          ]
        }
      ],
      "source": [
        "# Save the trained model\n",
        "model.save('ticket_classifier_model.keras')\n",
        "print(\"Model saved as 'ticket_classifier_model.keras'\")\n",
        "\n",
        "# Save tokenizer\n",
        "with open('ticket_tokenizer.pkl', 'wb') as f:\n",
        "    pickle.dump(tokenizer, f)\n",
        "print(\"Tokenizer saved as 'ticket_tokenizer.pkl'\")\n",
        "\n",
        "# Save label encoder\n",
        "with open('ticket_label_encoder.pkl', 'wb') as f:\n",
        "    pickle.dump(label_encoder, f)\n",
        "print(\"Label encoder saved as 'ticket_label_encoder.pkl'\")\n",
        "\n",
        "# Save preprocessing parameters\n",
        "preprocessing_params = {\n",
        "    'max_sequence_length': MAX_SEQUENCE_LENGTH,\n",
        "    'max_words': MAX_WORDS,\n",
        "    'stop_words': list(stop_words)\n",
        "}\n",
        "\n",
        "with open('ticket_preprocessing_params.pkl', 'wb') as f:\n",
        "    pickle.dump(preprocessing_params, f)\n",
        "print(\"Preprocessing parameters saved as 'ticket_preprocessing_params.pkl'\")\n",
        "\n",
        "print(\"\\nAll artifacts saved successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hCYrHnAAdHg3"
      },
      "source": [
        "## Step 14: Test Prediction Function\n",
        "\n",
        "**Task:** Create a prediction function for ticket classification and test it with sample tickets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "58CDh3aIdHg3",
        "outputId": "c3cba418-1d8f-45e7-dafe-eeecd7b347af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing prediction function:\n",
            "\n",
            "Ticket: My application keeps crashing when I try to export data. Need urgent technical help!\n",
            "Predicted Department: Technical Support\n",
            "Confidence: 0.7068\n",
            "All probabilities: {'Billing': np.float32(0.075473905), 'Customer Service': np.float32(0.13059103), 'Sales': np.float32(0.0871725), 'Technical Support': np.float32(0.70676255)}\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Ticket: I would like to know more about your enterprise pricing plans and volume discounts.\n",
            "Predicted Department: Customer Service\n",
            "Confidence: 0.3164\n",
            "All probabilities: {'Billing': np.float32(0.19953302), 'Customer Service': np.float32(0.316415), 'Sales': np.float32(0.24276756), 'Technical Support': np.float32(0.2412844)}\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Ticket: I was charged twice on my credit card for the same subscription. Please refund the duplicate charge.\n",
            "Predicted Department: Customer Service\n",
            "Confidence: 0.3124\n",
            "All probabilities: {'Billing': np.float32(0.19597958), 'Customer Service': np.float32(0.3123561), 'Sales': np.float32(0.23891607), 'Technical Support': np.float32(0.2527483)}\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Ticket: Thank you for the excellent support! My issue was resolved quickly and professionally.\n",
            "Predicted Department: Customer Service\n",
            "Confidence: 0.3169\n",
            "All probabilities: {'Billing': np.float32(0.1960436), 'Customer Service': np.float32(0.31689972), 'Sales': np.float32(0.24246107), 'Technical Support': np.float32(0.2445956)}\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Ticket: Cannot login to my account. Password reset is not working. This is urgent!\n",
            "Predicted Department: Technical Support\n",
            "Confidence: 0.8245\n",
            "All probabilities: {'Billing': np.float32(0.043332405), 'Customer Service': np.float32(0.08109997), 'Sales': np.float32(0.05103035), 'Technical Support': np.float32(0.8245373)}\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Ticket: What are the contract terms for annual subscriptions? Need information before purchase.\n",
            "Predicted Department: Customer Service\n",
            "Confidence: 0.3170\n",
            "All probabilities: {'Billing': np.float32(0.19770935), 'Customer Service': np.float32(0.31695482), 'Sales': np.float32(0.2428825), 'Technical Support': np.float32(0.24245338)}\n",
            "----------------------------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "def predict_department(text, model, tokenizer, label_encoder, max_len):\n",
        "    \"\"\"\n",
        "    Predict department for a given support ticket\n",
        "    \"\"\"\n",
        "    # Preprocess text\n",
        "    cleaned = clean_text(text)\n",
        "    processed = tokenize_and_lemmatize(cleaned)\n",
        "\n",
        "    # Tokenize and pad\n",
        "    sequence = tokenizer.texts_to_sequences([processed])\n",
        "    padded = pad_sequences(sequence, maxlen=max_len, padding='post', truncating='post')\n",
        "\n",
        "    # Predict\n",
        "    prediction = model.predict(padded, verbose=0)\n",
        "    predicted_class = np.argmax(prediction, axis=1)[0]\n",
        "    confidence = prediction[0][predicted_class]\n",
        "\n",
        "    # Get department label\n",
        "    department = label_encoder.classes_[predicted_class]\n",
        "\n",
        "    return department, confidence, prediction[0]\n",
        "\n",
        "# Test with sample tickets\n",
        "test_tickets = [\n",
        "    \"My application keeps crashing when I try to export data. Need urgent technical help!\",\n",
        "    \"I would like to know more about your enterprise pricing plans and volume discounts.\",\n",
        "    \"I was charged twice on my credit card for the same subscription. Please refund the duplicate charge.\",\n",
        "    \"Thank you for the excellent support! My issue was resolved quickly and professionally.\",\n",
        "    \"Cannot login to my account. Password reset is not working. This is urgent!\",\n",
        "    \"What are the contract terms for annual subscriptions? Need information before purchase.\"\n",
        "]\n",
        "\n",
        "print(\"Testing prediction function:\\n\")\n",
        "for ticket in test_tickets:\n",
        "    department, confidence, probs = predict_department(\n",
        "        ticket, model, tokenizer, label_encoder, MAX_SEQUENCE_LENGTH\n",
        "    )\n",
        "    print(f\"Ticket: {ticket}\")\n",
        "    print(f\"Predicted Department: {department}\")\n",
        "    print(f\"Confidence: {confidence:.4f}\")\n",
        "    print(f\"All probabilities: {dict(zip(label_encoder.classes_, probs))}\")\n",
        "    print(\"-\" * 100)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}